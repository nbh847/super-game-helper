# 混合策略数据收集与训练方案

## 📌 方案概述

**核心思路**：采用渐进式策略，V1专注常用英雄训练专精模型，V2+扩展到多英雄泛化模型。

**适用场景**：
- 大乱斗模式英雄随机分配
- 无法保证获得指定英雄
- 希望快速启动项目，同时支持渐进式扩展

**优势**：
- ✅ V1快速见效（专注2-3个英雄）
- ✅ 不需要等待特定英雄，周期可控
- ✅ V2+逐步扩展到多英雄
- ✅ 风险可控，迭代优化

---

## 🎯 阶段划分

### 阶段0：英雄分析（第1-3天）

**目标**：确定训练目标英雄和类型

**任务**：
1. 统计最近50局大乱斗的英雄分布
2. 找出出现频率最高的3-5个英雄
3. 选择其中2-3个你擅长的英雄
4. 分析英雄类型（坦克/法师/射手/刺客/辅助）

**输出**：
- 目标英雄列表（2-3个）
- 英雄类型分布
- 预估数据收集周期

---

### 阶段1：常用英雄专精模型（第4-25天）

**目标**：训练V1版本，AI能玩2-3个常用英雄

#### 1.1 数据收集（第4-20天）

**录制目标**：
- 英雄A：15-20局
- 英雄B：15-20局
- 英雄C：可选（10-15局）
- 总计：40-55局

**数据筛选标准**：
- ✅ 胜局：60-70%
- ✅ 高质量败局：30-40%（KDA>1.0，对局>10分钟）
- ❌ 剔除低质量败局（KDA<0.5，死亡>15次）
- ❌ 剔除异常对局（挂机、掉线）

**录制策略**：
- 正常玩大乱斗，不刻意追求特定英雄
- 碰到目标英雄时认真打
- 开启自动录像功能
- 记录每局基本信息（英雄、KDA、胜率、对局时长）

**预计时间**：
- 每天1-3局
- 假设2个目标英雄出现概率50%
- 累计40-55局需要17-20天

#### 1.2 数据处理（第21-22天）

**视觉编码器训练数据**：
- 从每局提取200帧（每1秒1帧）
- 人工标注英雄状态（移动/攻击/技能/受伤/死亡）
- 总计：8000-11000帧

**DQN训练数据**：
- 从每局提取experience（状态+动作+奖励）
- 每局约100-150个experience
- 总计：4000-8250个experience

#### 1.3 模型训练（第23-25天）

**视觉编码器训练**：
- 训练数据：8000-11000帧
- 模型：小型CNN（0.71M参数）
- 预期准确率：70-80%（一般水平数据）
- 训练时间：1-2小时

**DQN离线训练**：
- 训练数据：4000-8250个experience
- 模型：DQN网络（43K参数）
- 训练轮数：100-200轮
- 预期平均reward：逐步提升
- 训练时间：2-4小时

**预期效果**：
- ✅ AI能够识别游戏状态（视觉编码器）
- ✅ AI能够进行基础决策（DQN）
- ✅ AI能够执行操作（移动、攻击、回城）
- ✅ 胜率：40-50%（人机对战）
- ⚠️ 水平：一般，操作合理但策略有限
- 🎯 支持英雄：2-3个目标英雄

---

### 阶段2：多英雄泛化模型（第26天后，可选）

**目标**：扩展模型支持更多英雄

#### 2.1 数据收集（第26-50天）

**录制目标**：
- 每个英雄类型积累10-15局
- 类型分布：
  - 坦克：10-15局
  - 法师：10-15局
  - 射手：10-15局
  - 刺客：10-15局（可选）
  - 辅助：10-15局（可选）

**数据筛选**：
- 同阶段1的筛选标准
- 胜局：50-60%
- 高质量败局：40-50%

**预计时间**：
- 每天1-3局
- 假设每类型英雄出现概率20%
- 累计50-75局需要25-35天

#### 2.2 模型升级（第51-55天）

**模型架构调整**：
- 输入不仅包括视觉特征
- 加入英雄特征（类型、攻击距离、技能CD等）
- 训练模型学习英雄类型的通用策略

**训练策略**：
- 迁移学习：加载阶段1的专精模型
- 冻结视觉编码器
- 微调DQN网络（适应多英雄）

**预期效果**：
- ✅ 支持更多英雄类型（4-5个类型）
- ✅ 泛化能力提升
- ✅ 胜率：45-55%（人机对战）
- ⚠️ 水平：中等，能够适应不同英雄

---

### 阶段3：在线学习优化（持续进行）

**目标**：让AI在实战中持续学习

#### 3.1 在线学习实施

**实施方法**：
1. 加载离线训练的模型
2. 进入真实对局
3. 实时决策和记录
4. 每局后更新模型
5. 逐步提升水平

**在线学习配置**：
- ε值（探索率）：0.05-0.1（保守探索）
- 每局保存一次模型
- 定期备份（每5局）
- 紧急停止机制

**预期效果**：
- 前10-20局：AI水平一般，能正常操作
- 20-50局后：逐渐适应真实对局
- 50-100局后：达到中等水平

---

## 📊 详细时间表

| 阶段 | 天数 | 局数 | 任务 | 成果 |
|------|------|------|------|------|
| **阶段0** | 1-3天 | - | 英雄分析 | ✅ 确定目标英雄 |
| **阶段1** | 4-25天 | 40-55局 | 录制+训练专精模型 | ✅ AI能玩2-3个英雄 |
| **阶段2** | 26-55天 | 50-75局 | 录制+训练泛化模型 | ✅ AI能玩多英雄类型 |
| **阶段3** | 持续 | - | 在线学习优化 | ✅ AI逐步提升水平 |

**总计**（仅阶段0-1）：
- 录制时间：20-23天
- 训练时间：2-3小时
- 总周期：23-28天

**总计**（阶段0-2）：
- 录制时间：45-53天
- 训练时间：3-5小时
- 总周期：50-60天

---

## 💾 数据存储需求

### 阶段1（专精模型）
- 原始录像：40-55局 × 20MB/局 = 800MB-1.1GB
- 提取帧：40-55局 × 200帧 = 8000-11000张图片（约400-550MB）
- Experience数据：4000-8250个experience（约50-100MB）
- 模型文件：100-200MB

### 阶段2（泛化模型）
- 原始录像：50-75局 × 20MB/局 = 1-1.5GB
- Experience数据：新增experience（约50-150MB）
- 模型文件：150-300MB

### 总存储需求
- **最小**（仅阶段1）：约1.5-2GB
- **推荐**（阶段1+2）：约3-4GB
- **完整**（含备份和日志）：预留5-10GB

---

## 🎯 各阶段成果预期

### 阶段1：V1版本（能玩就行）
- ✅ AI能够识别游戏状态（视觉编码器）
- ✅ AI能够进行基础决策（DQN）
- ✅ AI能够执行操作（移动、攻击、回城）
- ✅ 胜率：40-50%（人机对战）
- ✅ 支持英雄：2-3个目标英雄
- ⚠️ 水平：一般，操作合理但策略有限

### 阶段2：V2版本（多英雄）
- ✅ 支持更多英雄类型（4-5个类型）
- ✅ 泛化能力提升
- ✅ 胜率：45-55%（人机对战）
- ✅ 水平：中等，能够适应不同英雄

### 阶段3：V3+版本（在线学习优化后）
- ✅ 适应真实玩家对局
- ✅ 学习对手风格
- ✅ 逐步提升到中等水平
- ✅ 胜率：50-60%（人机对战）
- ✅ 持续优化

---

## 📋 实施检查清单

### 阶段0：英雄分析
- [ ] 统计最近50局大乱斗的英雄分布
- [ ] 找出出现频率最高的3-5个英雄
- [ ] 选择其中2-3个你擅长的英雄
- [ ] 分析英雄类型（坦克/法师/射手/刺客/辅助）
- [ ] 确定数据收集周期

### 阶段1：专精模型
#### 数据收集
- [ ] 开启游戏自动录像
- [ ] 每天录制1-3局
- [ ] 记录每局信息（英雄、KDA、胜率）
- [ ] 累计达到40-55局

#### 数据处理
- [ ] 提取帧并标注（label_tool.py）
- [ ] 训练视觉编码器
- [ ] 提取experience
- [ ] 训练DQN模型
- [ ] 保存模型文件

#### 测试验证
- [ ] 测试视觉编码器准确率
- [ ] 测试DQN决策合理性
- [ ] 端到端测试（1局大乱斗）
- [ ] 生成测试报告

### 阶段2：泛化模型（可选）
#### 数据收集
- [ ] 继续录制，累计达到50-75局
- [ ] 按英雄类型分组
- [ ] 每个类型10-15局

#### 模型升级
- [ ] 加入英雄特征
- [ ] 迁移学习训练
- [ ] 测试泛化能力
- [ ] 保存新模型

### 阶段3：在线学习
- [ ] 加载离线训练模型
- [ ] 进入真实对局
- [ ] 开启在线学习
- [ ] 每局更新模型
- [ ] 定期备份
- [ ] 持续优化

---

## ⚠️ 注意事项

### 数据质量优先
- ✅ 宁可少录高质量的局，不要录大量低质量的局
- ✅ 剔除送人头、挂机的局
- ✅ 优先选择高质量败局（KDA>1.0）
- ✅ 记录详细的对局信息

### 分阶段实施
- ✅ 先完成阶段1（V1），再考虑阶段2
- ✅ 每个阶段充分测试再推进
- ✅ 不要一次性追求完美

### 持续备份
- ✅ 每天备份录像文件
- ✅ 训练后备份模型文件
- ✅ 定期备份到云盘或外部硬盘
- ✅ 避免数据丢失

### 记录日志
- ✅ 记录每局的关键信息
- ✅ 记录训练过程中的问题
- ✅ 记录模型性能变化
- ✅ 方便后续分析

### 灵活调整
- ✅ 根据实际数据情况调整策略
- ✅ 如果某些英雄出现率低，可以替换目标英雄
- ✅ 训练效果不好时，及时调整数据筛选标准
- ✅ 迭代优化比完美更重要

---

## 🔧 工具和脚本

### 数据收集
- 游戏内置自动录像功能
- 自定义录像管理脚本（可选）
- 对局信息记录脚本（可选）

### 数据处理
- `backend/scripts/label_tool.py`：标注工具
- `backend/ai/models/hero_state_dataset.py`：数据加载器
- `backend/ai/models/replay_buffer_offline.py`：经验回放缓冲区

### 模型训练
- `backend/train_state_classifier.py`：视觉编码器训练
- `backend/train_dqn.py`：DQN训练脚本
- `backend/scripts/inference.py`：推理脚本

### 在线学习
- `backend/ai/finetune/online_collector.py`：在线数据收集（待实现）
- `backend/ai/finetune/online_trainer.py`：在线DQN微调（待实现）

---

## 📚 参考文档

- [数据收集方案](data_collection.md)
- [设计提案](design_proposal.md)
- [开发进度](progress.md)
- [架构设计](architecture.md)

---

**文档版本**: 1.0
**创建日期**: 2026-01-21
**最后更新**: 2026-01-21
**作者**: Claude AI
**状态**: 已完成
