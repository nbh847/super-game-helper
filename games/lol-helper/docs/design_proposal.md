# 设计提案

## 项目概述

本项目旨在开发一个英雄联盟极地大乱斗模式的 AI 助手，通过行为克隆和强化学习技术，训练能够自主操作的 AI 玩家。项目强调模拟人类操作行为，避免被游戏官方检测为外挂。

## 核心目标

- 全自动操作极地大乱斗模式
- 通过 AI 学习和强化游戏能力
- 模拟人类操作，降低检测风险
- 支持全英雄分类（坦克/法师/射手/刺客/辅助）
- 优先战斗策略，兼顾补刀

## 技术路线

- 预训练（行为克隆）+ 微调（强化学习）
- 渐进式技能复杂度提升
- 基于现有 .rofl 游戏录像数据训练

### 技术选型依据

**调研总结**（2026年1月）：

1. **Grok AI 案例分析**
   - ✅ 92%胜率验证技术路线可行性
   - ✅ 行为克隆+强化学习组合有效
   - ✅ 多英雄泛化能力（22个英雄）
   - ⚠️ 过于规律的作息和高胜率易被关注
   - ⚠️ 需要更严格的人类行为模拟

2. **AlphaStar 技术方案**
   - ✅ Actor-Critic + LSTM + PPO 组合
   - ✅ 多智能体自我对弈
   - ✅ 渐进式训练方法

3. **开源项目验证**
   - TLoL：完整数据处理流程
   - LeagueAI：图像识别框架
   - YOLOv8n：快速目标检测（适合实时）

**技术栈选择**：

| 模块 | 技术选型 | 理由 |
|------|---------|------|
| 录像解析 | lol-replay-parser | 成熟、易用 |
| 图像识别 | YOLOv8n + PaddleOCR | YOLOv8n快且小，适合个人配置 |
| 行为克隆 | PyTorch + 自定义模型 | 灵活、易调试 |
| 强化学习 | Stable-Baselines3 (PPO) | 成熟、文档完善 |
| 操作模拟 | PyAutoGUI + pynput | 跨平台、简单 |

## 实施计划

### 阶段 1：基础框架搭建
- [ ] 创建目录结构
- [ ] 配置文件模板
- [ ] 基础模块骨架
- [ ] 依赖库安装和测试

### 阶段 2：数据处理
- [ ] 录像解析模块
- [ ] 状态/动作提取
- [ ] 数据加载器
- [ ] 数据增强测试

### 阶段 3：模型训练（V1）
- [ ] 策略网络模型
- [ ] 预训练器
- [ ] 基础操作训练
- [ ] 模型评估

### 阶段 4：实时游戏识别
- [ ] 屏幕截取优化
- [ ] 图像识别模块
- [ ] 游戏状态识别
- [ ] 实时性能优化

### 阶段 5：操作执行器
- [ ] 鼠标/键盘模拟
- [ ] 人类行为模拟
- [ ] 防检测机制
- [ ] 操作准确性测试

### 阶段 6：集成与测试
- [ ] 端到端集成
- [ ] 实战测试
- [ ] 性能优化
- [ ] 错误处理和日志

### 阶段 7：扩展（V2+）
- [ ] 技能释放模块
- [ ] 更复杂策略
- [ ] 强化学习微调
- [ ] 新英雄支持

## 渐进式技能复杂度

### V1：基础操作（移动 + 普攻）
- 基础走位
- 补刀（小兵）
- 识别安全距离

**目标**：验证整体流程，训练快速，显存占用低（~1-2GB）

### V2：+ 1 个主要技能
- 识别技能命中范围
- 合理时机释放
- 结合普攻输出

**目标**：技能释放决策，提升战斗能力

### V3：+ 2-3 个技能组合
- 技能连招
- 技能搭配使用
- 冷却时间管理

**目标**：复杂技能配合，提升上限

### V4：完整技能 + 召唤师技能
- 复杂连招
- 召唤师技能使用
- 全技能配合

**目标**：完整技能系统，接近人类高水平

## 战斗优先策略

### 优先级顺序
1. **生存**：血量低时优先撤退/回血
2. **团战**：敌方英雄集结时优先战斗
3. **击杀机会**：敌方残血时优先追击
4. **输出**：在安全位置输出伤害
5. **补刀**：空闲时补刀积累经济

### 威胁度评估
- 敌方英雄血量
- 敌方英雄技能冷却
- 敌方英雄与己方距离
- 防御塔覆盖范围

### 团战策略
- **坦克**：前排承伤，控制敌方
- **法师**：后排输出，范围技能
- **射手**：安全位置，持续输出
- **刺客**：切入后排，击杀脆皮
- **辅助**：保护队友，提供增益

## 英雄支持策略

### 针对少数英雄的策略扩展

**阶段 1：基础模型**
- 使用现有英雄数据训练基础模型
- 支持已有英雄的完整策略

**阶段 2：数据增强**
- 镜像翻转（左右对称）
- 位置平移（小范围）
- 时间扭曲（速度调整）

**阶段 3：迁移学习**
- 将基础模型迁移到新英雄
- 共享底层特征提取器
- 冻结部分网络层

**阶段 4：少量新英雄数据微调**
- 收集少量新英雄对局数据
- 微调最后几层网络
- 快速适配新英雄

## 风险与挑战

### 技术挑战
1. **实时性能**：屏幕识别和决策需要快节奏响应
2. **状态不完整**：屏幕截图信息有限，需要推理
3. **数据不足**：少数英雄数据需要数据增强和迁移学习
4. **泛化能力**：不同场景和对手的适应

### 风险控制
1. **分阶段验证**：每个阶段充分测试再推进
2. **模块化设计**：各模块独立开发和测试
3. **渐进式复杂度**：从简单到复杂逐步提升
4. **持续监控**：运行时记录日志和性能指标

### 个人用户方案（RTX5060 8GB）

**简化策略**：
- 只支持大乱斗模式（简化状态空间）
- 专注于10-15个常用英雄
- V1阶段先训练基础操作（移动+普攻）
- 行为克隆为主，强化学习为辅
- 总显存控制在4GB以内

**模型优化**：
```python
# 针对个人配置的优化
MODEL_CONFIG = {
    'hidden_size': 128,           # 隐藏层大小（降低）
    'cnn_channels': [32, 64, 128], # CNN通道数（减少）
    'lstm_hidden': 128,           # LSTM隐藏层（降低）
    'batch_size': 16,             # 批次大小（降低）
    'frame_skip': 4,              # 每4帧处理一次
    'frame_stack': 4,             # 堆叠4帧
}
```

**数据需求**：
- 每个英雄：10-20局大乱斗录像
- 总计：150-300局录像
- 或使用TLoL/HuggingFace数据集

**开发时间**（个人）：
- 阶段1-2：框架+数据（3-5天）
- 阶段3：V1训练（3-5天）
- 阶段4-5：识别+执行（3-5天）
- 阶段6：集成测试（2-3天）
- **总计：11-18天**

## 资源需求评估

### 硬件配置
- **RTX5060 8GB**：中端显卡，8GB 显存足够
- **16GB 内存**：标准配置，够用

### 各阶段资源需求

**阶段 1-3：数据处理和基础模型（V1）**
- 录像解析：CPU 为主，内存占用 < 4GB
- 小规模模型训练：显存占用 ~2-4GB
- ✅ **配置充足**

**阶段 4-5：实时游戏识别和操作**
- 屏幕识别推理：显存占用 ~1-2GB
- CPU 处理游戏状态：内存占用 ~2-4GB
- ✅ **完全够用**

**阶段 6-7：强化学习和复杂模型（V2+）**
- 大模型训练：显存占用 5-7GB（可能接近上限）
- ✅ **基本够用，需调优参数**

### 优化建议

如果遇到资源不足，可以这样优化：

**降低显存占用**
```python
# 减小 batch size
batch_size = 16  # 原来 32

# 使用混合精度训练
from torch.cuda.amp import autocast

# 减小模型规模
hidden_size = 256  # 原来 512
```

**降低内存占用**
```python
# 数据分批加载
dataloader = DataLoader(..., batch_size=32, num_workers=2)

# 及时释放内存
del data
torch.cuda.empty_cache()
```

## 后续优化方向

1. **多模态输入**：结合游戏 API 数据（如果可用）
2. **对手建模**：学习和适应不同对手风格
3. **团队协作**：多 AI 配合（模拟团队对局）
4. **在线学习系统**：实战中持续优化策略（重要优化方向）

   ### 在线学习实施计划

   **阶段1：数据收集**
   - 真实对局记录（状态、动作、奖励序列）
   - 在线经验回放（与离线buffer整合）
   - 对手风格识别（激进/保守/中等）
   - 自动筛选高质量对局

   **阶段2：在线微调**
   - 加载离线预训练模型
   - 实时经验回放更新
   - 在线TD误差计算
   - 定期保存模型（每局保存一次）

   **阶段3：防检测强化**
   - 严格的APM控制（150-250）
   - 动态操作延迟（200-600ms随机）
   - 偶尔"失误"模拟（5-10%概率）
   - 定期"休息"机制（每局暂停1-3秒）

   **阶段4：安全措施**
   - 紧急停止机制（快捷键触发）
   - 训练时长限制（每局最多20分钟）
   - 模型自动备份和回滚
   - 性能监控和评估

   **实施阶段**：
   - **V1版本**：仅离线训练（不启动在线学习）
   - **V2版本**：在线学习辅助（低ε值0.05-0.1）
   - **V3+版本**：在线学习主要（ε值0.1-0.2）
   - 从低段位开始，逐步提升段位

   **预期效果**：
   - ✅ 自适应不同对手风格
   - ✅ 发现并弥补录像中的盲区
   - ✅ 持续提升实战能力
   - ✅ 保持人类行为特征

   **注意事项**：
   - ⚠️ 必须严格防检测（人类行为模拟）
   - ⚠️ 建议先在低段位测试
   - ⚠️ 限制训练时长和频率
   - ⚠️ 定期备份模型，避免策略退化

5. **可视化分析**：训练和操作过程可视化
6. **配置化管理**：支持灵活调整策略参数

## 相关文档

- [架构设计](architecture.md) - 详细的技术架构和模块说明
- [防检测策略](anti_detection.md) - 完整的防检测实施方案
- [模块API参考](api_reference.md) - 代码接口和实现细节

---

**文档版本**: 1.2
**最后更新**: 2026-01-21
**更新内容**：添加技术选型依据、个人用户简化方案；详细规划在线学习系统
